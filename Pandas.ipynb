{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPF1 | Course Day 2 | 18.05.2019\n",
    "\n",
    "# Using Pandas to Get More out of Data\n",
    "## Pandas\n",
    "* Pandas is a newer package built on top of NumPy\n",
    "* NumPy is very useful for numerical computing tasks* \n",
    "* Pandas allows more flexibility: Attaching labels to data, working with missing data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 30\n",
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # We will need NumPy throughout this course day\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas Objects\n",
    "* Pandas objects are enhanced versions of NumPy arrays: The rows and columns are identified with labels rather than simple integer indices\n",
    "* `Series` object: A one-dimensional array of indexed data\n",
    "* `DataFrame` object: A two-dimensional array with both flexible row indices and flexible column names\n",
    "\n",
    "## The Pandas `Series` Object\n",
    "* A Pandas `Series` object is a one-dimensional array of indexed data\n",
    " * NumPy array: has an _implicitly_ defined integer index\n",
    " * A `Series` object uses by default integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A `Series` object can have an _explicitly_ defined index associated with the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can access the index labels by using the `index` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Python dictionary maps arbitrary keys to a set of arbitrary values\n",
    "* A `Series` object maps _typed_ keys to a set of _typed_ values\n",
    " * \"Typed\" means we know the type of the indices and elements beforehand, making Pandas Series objects much more efficient than Python dictionaries for certain operations\n",
    "* We can construct a `Series` object directly from a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Note_: The index for the `Series` is drawn from the sorted keys\n",
    "\n",
    "## The Pandas `DataFrame` Object\n",
    "* A `DataFrame` object is an analog of a two-dimensional array both with flexible row indices and flexible column names\n",
    " * Both the rows and columns have a generalized index for accessing the data\n",
    " * The row indices can be accessed by using the `index` attribute\n",
    " * The column indices can be accessed by using the `columns` attribute\n",
    " \n",
    "### Constructing `DataFrame` Objects\n",
    "* You can think of a `DataFrame` as a sequence of aligned `Series` objects, meaning that each column of a `DataFrame` is a `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are multiple ways to construct a `DataFrame` object\n",
    " * From a single `Series` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * From a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * From a dictionary of `Series` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * From a two-dimensional NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection in `Series`\n",
    "\n",
    "`Series` as a dictionary: \n",
    " * Select elements by key, e.g. `data['a']`\n",
    " * Modify the `Series` object with familiar syntax, e.g. `data['e'] = 100`\n",
    " * Check if a key exists by using the `in` operator\n",
    " * Access all the keys by using the `keys()` method\n",
    " * Access all the values by using the `items()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Series` as one-dimensional array: \n",
    " * Select elements by the implicit integer index, e.g. `data[0]`\n",
    " * Select elements by the explicit index, e.g. `data['a']`\n",
    " * Select slices (by using an implicit integer index or an explicit index)\n",
    "   * _Important_: Slicing with an explicit index (e.g., `data['a':'c']`) will _include_ the final index in the slice, while slicing with an implicit index (e.g., `data[0:3]`) will _exclude_ the final index from the slice\n",
    " * Use masking operations, e.g., `data[data < 3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection in `DataFrame`\n",
    "* `DataFrame` as a dictionary of related `Series` objects: \n",
    " * Select `Series` by the column name, e.g. `df['area']`\n",
    " * Modify the `DataFrame` object with familiar syntax, e.g. `df['c3'] = df['c2']/ df['c1']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `DataFrame` as two-dimensional array: \n",
    " * Access the underlying NumPy data array by using the `values` attribute\n",
    "   * `df.values[0]` will select the first row\n",
    " * Use the `iloc` indexer to index, slice, and modify the data by using the implicit integer index\n",
    " * Use the `loc` indexer to index, slice, and modify the data by using the explicit index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ufuncs and Pandas\n",
    "* Pandas is designed to work with Numpy, thus any NumPy ufunc will work on Pandas Series and `DataFrame` objects\n",
    "* _Index preservation_: Indices are preserved when a new Pandas object will come out after applying ufuncs\n",
    "* _Index alignment_: Pandas will align indices in the process of performing an operation\n",
    " * Missing data is marked with `NaN` (\"Not a Number\")\n",
    " * We can specify on how to fill value for any elements that might be missing by using the optional keyword fill_value: `A.add(B, fill_value=0)`\n",
    " * We can also use the `dropna()` method to drop missing values\n",
    "* _Note_: Any of the ufuncs discussed for NumPy can be used in a similar manner with Pandas objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ufuncs: Index Preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ser = pd.Series(rng.randint(0, 10, 4))\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rng.randint(0, 10, (3, 4)),\n",
    "                  columns=['A', 'B', 'C', 'D'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ufuncs: Index Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.Series({'Alaska': 1723337, 'Texas': 695662,\n",
    "                  'California': 423967}, name='area')\n",
    "\n",
    "population = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
    "                        'New York': 19651127}, name='population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ufuncs: Operations Between DataFrame and Series\n",
    "* Operations between a `DataFrame` and a `Series` are similar to operations between a two-dimensional and one-dimensional NumPy array (e.g., compute the difference of a two-dimensional array and one of its rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(2)\n",
    "A = rng.randint(10, size=(3, 4))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading (and Writing) Data with Pandas\n",
    "### File Types\n",
    "* We will work with _plaintext files_ only in this session; these contain only basic text characters and do not include font, size, or color information\n",
    " * _Binary files_ are all other file types, such as PDFs, images, executable programs etc.\n",
    " \n",
    "### The Current Working Directory\n",
    "* Every program that runs on your computer has a _current working directory_\n",
    " * It's the directory from where the program is executed / run\n",
    " * _Folder_ is the more modern name for a directory\n",
    "* The _root_ directory is the top-most directory and is addressed by `/` \n",
    " * A directory `mydir1` in the root directory can be addressed by `/mydir1`\n",
    " * A directory `mydir2` within the `mydir1` directory can be address by `/mydir/mydir2`, and so on\n",
    " \n",
    "### Absolute and Relative Paths\n",
    "* An _absolute path_ begins always with the root folder, e.g. `/my/path/...`\n",
    "* A _relative path_ is always relative to the program's current working directory\n",
    " * If a program's current working directory is `/myprogram` and the directory contains a folder files with a file `test.txt`, then the relative path to that file is just `files/test.txt` \n",
    " * The absolute path to `test.txt` would be `/myprogram/files/test.txt` (note the root folder `/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls # List folder content for current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd # Print path to the current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data with Pandas\n",
    "* Pandas provides the `pandas.read_csv()` function to load data from a CSV file (or a file that uses a different delimiter than a comma)\n",
    " * The path you specify doesn't have to be on your hard disk; you can also provide the URL to a CSV file to read it directly into a Pandas object\n",
    " * We can set the optional argument `error_bad_lines` to `False` so that bad lines in the file get omitted and do not cause an error\n",
    " * Checkout the documentation to learn more about the optional arguments:<br>https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    " \n",
    "**Planets Data**: The _Planets_ dataset (available from the Seaborn package or the [`seaborn-data` repository](https://github.com/mwaskom/seaborn-data)) gives information on planets that astronomers have discovered around other stars (known as extrasolar planets or exoplanets for short). The file contains details on the 1000+ exoplanets discovered up to 2014.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planets = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/planets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Interesting Data Sources\n",
    "* Federal Statistical Office: https://www.bfs.admin.ch/bfs/en/home/statistics/catalogues-databases/data.html \n",
    "* OpenData: https://opendata.swiss/en/ \n",
    "* United Nations: http://data.un.org/ \n",
    "* World Health Organization: http://apps.who.int/gho/data/node.home \n",
    "* World Bank: https://data.worldbank.org/ \n",
    "* Kaggle: https://www.kaggle.com/datasets \n",
    "* Cern: http://opendata.cern.ch/\n",
    "* Nasa: https://data.nasa.gov/ \n",
    "* FiveThirtyEight: https://github.com/fivethirtyeight/data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating and Grouping Data in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Aggregation in Pandas\n",
    "* As with one-dimensional NumPy array, for a Pandas `Series` the aggregates return a single value\n",
    "* For a `DataFrame`, the aggregates return by default results within each column\n",
    "* Pandas `Series` and `DataFrames` include all of the common NumPy aggregates\n",
    " * In addition, there is a convenience method `describe()` that computes several common aggregates for each column and returns the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split, Apply, Combine\n",
    "* _Split_: Break up and group a DataFrame depending on the value of the specified key\n",
    "* _Apply_: Apply some function, usually an aggregate, transformation, or filtering, within the individual groups\n",
    "* _Combine_: Merge the results of these operations into an output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `GroupBy` Object\n",
    "* The `groupBy()` method returns a `DataFrameGroupBy`: It's a special view of the `DataFrame`\n",
    " * Helps get information about the groups, but does no actual computation until the aggregation is applied (\"lazy evaluation\", i.e. evaluate only when needed)\n",
    " * Apply an aggregate to this `DataFrameGroupBy` object: This will perform the appropriate apply/combine steps to produce the desired result\n",
    "   * You can apply any Pandas or NumPy aggregation function\n",
    " * Other important operations made available by a `GroupBy` are _filter_, _transform_, and _apply_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'], 'data': range(1,7)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Indexing and Iterating Over Groups\n",
    "* The `GroupBy` object supports column indexing in the same way as the `DataFrame`, and returns a modified `GroupBy` object\n",
    "* The `GroupBy` object also supports direct iteration over the groups, returning each group as a Series or `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate, Filter, Transform, and Apply\n",
    "* _Aggregate_: The `aggregate()` method can compute multiple aggregates at once\n",
    "* _Filter_: The `filter()` method allows you to drop data based on group properties\n",
    " * _Note_: `filter()` takes as an argument a function that returns a Boolean value specifying whether the group passes the filtering\n",
    "* _Transformation_: While aggregation must return a reduced version of the data, `transform()` can return some transformed version of the full data to recombine (meaning that we still have the same number of entries before and after the transformation)\n",
    "* _Apply_: The `apply()` method lets you apply an arbitrary function to the group results (or even to `DataFrame`s in general). The arbitrary function should take a `DataFrame`, and return either a Pandas object or a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(4)\n",
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0, 10, 6),\n",
    "                   'price': ['$ 123.00', '$ 112.00', '$ 123.00', '$ 12.32', '$ 14.32', '$ 0.123']})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform: an Example Based on Sales Data\n",
    "Source: http://pbpython.com/pandas_transform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"datasets/sales_transactions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
